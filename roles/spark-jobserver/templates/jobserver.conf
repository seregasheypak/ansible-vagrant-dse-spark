# Spark Cluster / Job Server configuration
spark {
  # spark.master will be passed to each job's JobContext
  #master = "spark://{% for host in groups['sparkmasters'] %}{{ host }}:7077{% if not loop.last %},{% endif %}{% endfor %}"
  master= "spark://{{ spark.sparkMaster }}:7077"
  # master = "mesos://vm28-hulk-pub:5050"
  # master = "yarn-client"

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 6

  jobserver {
    port = 8090
    jar-store-rootdir = {{jobserver.location}}/jars

    jobdao = spark.jobserver.io.JobFileDAO

    filedao {
      rootdir = {{jobserver.location}}/data
    }
  }

  # predefined Spark contexts
  # contexts {
  #   my-low-latency-context {
  #     num-cpu-cores = 1           # Number of cores to allocate.  Required.
  #     memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
  #   }
  #   # define additional contexts here
  # }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = {{ spark.workerCores }}           # Number of cores to allocate.  Required.
    memory-per-node = {{ spark.defaultExecutorMemory }}         # Executor memory per node, -Xmx style eg 512m, #1G, etc.

    # uris of jars to be loaded into the classpath for this context
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]
    spark.cassandra.connection.host="{% for host in groups['dse'] %}{{ host }}{% if not loop.last %},{% endif %}{% endfor %}"
    # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,
    # such as hadoop connection settings that don't use the "spark." prefix
    spark.eventLog.dir={{spark.eventLog}}
    spark.eventLog.enabled=true

    spark.ui.retainedStages=10
    spark.ui.retainedJobs=10
    spark.ui.killEnabled=true

    passthrough {
      #es.nodes = "192.1.1.1"
    }
  }

  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  # home = "/home/spark/spark"
}

# check the reference.conf in spray-can/src/main/resources for all defined settings
spray.can.server {
  # uncomment the next line for making this an HTTPS example
  # ssl-encryption = on
  idle-timeout = 60 s
  request-timeout = 40 s
  pipelining-limit = 2 # for maximum performance (prevents StopReading / ResumeReading messages to the IOBridge)
  # Needed for HTTP/1.0 requests with missing Host headers
  default-host-header = "spray.io:8765"

  # Increase this in order to upload bigger job jars
  parsing.max-content-length = 500m
}