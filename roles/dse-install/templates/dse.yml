 Memory limit for DSE In-Memory tables as a fraction of system memory (the default is 0.2, or 20%)
max_memory_to_lock_fraction: 0.20

# Can also be specified as a maximum in MB; if this is set to a non zero value the fraction value is ignored.
# max_memory_to_lock_mb: 10240

# Enable the Hive Meta Store via Cassandra
hive_meta_store_enabled: true

# Kerberos options
#
# The qop is the "Quality of Protection" for each connection.  Used by clients
# and servers.  Below is a list of valid values and their meanings.
#   auth - (default) authentication only
#   auth-int - authentication plus integity protection of all transmitted data
#   auth-conf - authetication plus integrity protection and encryption of all
#              transmitted data
# Warning: Encryption using auth-conf is separate and completely independent
# of whether encryption is done using SSL.  So that if auth-conf is selected
# here and SSL is enabled, then the transmitted data will be encrypted twice.
kerberos_options:
    keytab: resources/dse/conf/dse.keytab
    service_principal: dse/_HOST@REALM
    http_principal: HTTP/_HOST@REALM
    qop: auth

# LDAP options
#
# These are options will be used if the com.datastax.bdp.cassandra.auth.LdapAuthenticator
# is configured as the authenticator in cassandra.yaml
#
# ldap_options:
    # server_host: localhost
    # server_port: 389
    # DN of the user that be used to search for users on the LDAP server. This user should
    # only have the necessary permissions to do the search
    # If not present then an anonymous bind will be used for the search
    # search_dn: cn=Admin
    # Password of the search user
    # search_password: secret
    # Set to true to use an SSL encrypted connection. In this case the server_port needs
    # to be set to the ldaps port for the server
    # use_ssl: false
    # Set to true to initiate a TLS encrypted connection on the default ldap port
    # use_tls: false
    # truststore_path:
    # truststore_password:
    # truststore_type: jks
    # user_search_base: ou=users,dc=example,dc=com
    # user_search_filter: (uid={0})
    # Validity period for the credentials cache in milli-seconds (remote bind is an expensive
    # operation). Defaults to 0, set to 0 to disable.
    # credentials_validity_in_ms: 0
    # Validity period for the search cache in seconds. Defaults to 0, set to 0 to disable.
    # search_validity_in_seconds: 0
    # connection_pool:
        # max_active: 8
        # max_idle: 8

# To ensure that data with a TTL is purged from Solr indexes when it expires,
# DSE periodically checks indexes for data that has exceeded its TTL. These settings 
# control the scheduling & execution of those checks.
ttl_index_rebuild_options:
    # by default, schedule a check every 300 seconds
    fixed_rate_period: 300
    # the first check is delayed to speed up startup time
    initial_delay: 20
    # documents subject to TTL are checked in batches: this configures the max number of docs
    # checked per batch 
    max_docs_per_batch: 200

# Solr shard transport options, for inter-node communication between Solr nodes.
shard_transport_options:
#  
# Default type from 4.5.0 onwards is "netty" for TCP-based communication, 
# providing lower latency, improved throughput and reduced resource consumption.
# The other type is "http" for plain old Solr communication via the standard 
# HTTP-based interface.
    type: netty
#
# Options specific to the "netty" transport type.
#
# The TCP listen port, mandatory if you either want to use the "netty" transport
# type, or want to later migrate to it from the "http" one. If you plan to use
# and stay with the "http" one, either comment it out or set it to -1.
    netty_server_port: 8984
# The number of server acceptor threads (default is number of available processors). 
#   netty_server_acceptor_threads: 
# The number of server worker threads (default is number of available processors * 8). 
#   netty_server_worker_threads: 
# The number of client worker threads (default is number of available processors * 8). 
#   netty_client_worker_threads: 
# The max number of client connections (default is 100). 
#   netty_client_max_connections: 
# The cumulative shard request timeout, in milliseconds (default is 60000).
#   netty_client_request_timeout:
#
# Options specific to the "http" transport type.
#
# HTTP shard client timeouts in milliseconds. 
# Default is the same as Solr, that is 0, meaning no timeout at all; it is
# strongly suggested to change it to a finite value, to avoid blocking operations.
# Notice these settings are valid across Solr cores.
#   http_shard_client_conn_timeout: 0
#   http_shard_client_socket_timeout: 0
    
# Solr indexing settings
#
# Max number of concurrent asynchronous indexing threads per Solr core.
# Default is "number of available processors" * 2; if set at 1,
# the system reverts to the synchronous behavior, where data is
# synchronously written into Cassandra and indexed by Solr.
#
# max_solr_concurrency_per_core: 2
#
# The back pressure threshold is the total number of queued asynchronous
# indexing requests per core, computed at Solr commit time; 
# when exceeded, back pressure kicks in to avoid excessive 
# resources consumption, causing throttling of new incoming requests.
# Default is 500.
#
# back_pressure_threshold_per_core: 500
#
# The max time to wait for flushing of async index updates, happening either
# at Solr commit time or Cassandra flush time.
# Flushing should always complete successfully, in order to fully sync Solr indexes
# with Cassandra data, so should always be set at a reasonable high value, 
# expressed in minutes.
# Default is 5.
#
# flush_max_time_per_core: 5
#
# The max time to wait for each Solr core to load upon startup or create/reload operations, expressed in minutes.
# This is an advanced option, which should be changed only if any exceptions happen during core loading.
# Default is 1.
# load_max_time_per_core: 1

# Applies the configured Cassandra disk failure policy to index write failures.
# Default is disabled (false).
#
# enable_index_disk_failure_policy: false

# The directory to store index data; each Solr core index will be stored under 
# a solrconfig_data_dir/keyspace.table directory.
# Default is a solr.data directory inside Cassandra data directory, or as specified
# by the dse.solr.data.dir system property
#
#solr_data_dir: /MyDir

# Solr cql query options
#
# Max number of threads to use for retrieving rows during CQL Solr queries.
# This value is cross-request and cross-core.
# Default is "number of available processors" * 10.
#
# cql_solr_query_executor_threads: 2
#
# Max time in milliseconds to wait for each row to be read from Cassandra during
# CQL Solr queries.
# Default is 10000 (10 seconds).
#
# cql_solr_query_row_timeout: 10000

# CQL performance objects features:
# * CQL Slow Log
# * CQL System Info
# * User Level Latency Tracking
# * Resource Level Latency Tracking
# * Database Summary Statistics

# CQL slow log settings
cql_slow_log_options:
    enabled: false
    threshold_ms: 100
    ttl_seconds: 86400
    async_writers: 1

# CQL system info tables settings
cql_system_info_options:
    enabled: false
    refresh_rate_ms: 10000

# Data Resource latency tracking settings
resource_level_latency_tracking_options:
    enabled: false
    refresh_rate_ms: 10000

# Database summary stats options
db_summary_stats_options:
    enabled: false
    refresh_rate_ms: 10000

# Cluster summary stats options
cluster_summary_stats_options:
    enabled: false
    refresh_rate_ms: 10000
  
# Spark cluster summary stats options
spark_cluster_info_options:
    enabled: false
    refresh_rate_ms: 10000

# Spark application stats options
spark_application_info_options:
    enabled: false
    refresh_rate_ms: 10000
    driver:
        # enables or disables writing of the metrics collected at Spark Driver to Cassandra
        sink: false
        # enables or disables Spark Cassandra Connector metrics at Spark Driver
        connectorSource: false
        # enables or disables JVM heap and GC metrics at Spark Driver
        jvmSource: false
        # enables or disables application state metrics
        stateSource: false
    executor:
        # enables or disables writing of the metrics collected at executors to Cassandra
        sink: false
        # enables or disables Spark Cassandra Connector metrics at executors
        connectorSource: false
        # enables or disables JVM heap and GC metrics at executors
        jvmSource: false

# Column Family Histogram data tables options
histogram_data_options:
  enabled: false
  refresh_rate_ms: 10000
  retention_count: 3

# User/Resource latency tracking settings
user_level_latency_tracking_options:
   enabled: false
   refresh_rate_ms: 10000
   top_stats_limit: 100
   quantiles: false

# Solr Performance Objects

# Solr indexing error log options
solr_indexing_error_log_options:
    enabled: false
    ttl_seconds: 604800
    async_writers: 1
   
# Solr slow query log options
solr_slow_sub_query_log_options:
    enabled: false
    ttl_seconds: 604800
    async_writers: 1
    threshold_ms: 100

# Solr UpdateHandler metrics options
solr_update_handler_metrics_options:
    enabled: false
    ttl_seconds: 604800
    refresh_rate_ms: 60000

# Solr request handler metrics options
solr_request_handler_metrics_options:
    enabled: false
    ttl_seconds: 604800
    refresh_rate_ms: 60000

# Solr index statistics options
solr_index_stats_options:
    enabled: false
    ttl_seconds: 604800
    refresh_rate_ms: 60000

# Solr cache statistics options
solr_cache_stats_options:
    enabled: false
    ttl_seconds: 604800
    refresh_rate_ms: 60000
    
# Solr latency snapshot options
solr_latency_snapshot_options:
    enabled: false
    ttl_seconds: 604800
    refresh_rate_ms: 60000

# NodeHealth options
node_health_options:
    enabled: false
    refresh_rate_ms: 60000

# The directory where system keys are kept
#
# Keys used for sstable encryption must be distributed to all nodes
# Dse will need to be able to read and write to the directory.
#
# This directory should have 700 permissions and belong to the dse user
system_key_directory: /etc/dse/conf

# If this is set to true, DSE will expect the following config values to be encrypted:
#   resources/cassandra/conf/cassandra.yaml:
#     server_encryption_options.keystore_password
#     server_encryption_options.truststore_password
#     client_encryption_options.keystore_password
#     client_encryption_options.truststore_password
#   resources/dse/conf/dse.yaml:
#     ldap_options.search_password
#     ldap_options.truststore_password
#
# it's an error if the passwords aren't encrypted
#
# config values can be encrypted with dsetool encryptconfigvalue
config_encryption_active: false

# the name of the system key used to encrypt / decrypt passwords stored
# in configuration files.
#
# If config_encryption_active is true, it's an error if a valid key with
# this name isn't in the system key directory
#
# keyfiles, and KMIP managed keys can be created with dsetool createsystemkey
config_encryption_key_name: system_key

# Spark
# The fraction of available system resources to be used by Spark Worker
#
# This the only initial value, once it is reconfigured, the new value is stored
# and retrieved on next run.
initial_spark_worker_resources: 0.7

# Spark encryption can be enabled for Spark client to Spark cluster and Spark internode communication.
# It applies to 2 out of 4 communication protocols in Spark: control messages via Akka and file
# sharing via HTTP(S). It does not apply to RDD data exchange and to web UI. This means that
# the encryption is used to send all configuration settings and all files which are requires
# by Spark application. They include passwords and tokens.
#
# Spark encryption requires truststores to be defined.
#
# If truststore or keystore is provided as a relative file path, the base directory for them is
# Spark config directory denoted by SPARK_CONF_DIR environment variable (by default is points to
# resources/spark/conf).
spark_encryption_options:
    enabled: false
    keystore: .keystore
    keystore_password: cassandra
    key_password: cassandra
    truststore: .truststore
    truststore_password: cassandra
    # More advanced defaults below:
    # protocol: TLS
    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA]

# Audit logging options
audit_logging_options:
  enabled: false
  # The logger used for logging audit information
  # Available loggers are:
  #   CassandraAuditWriter: logs audit info to a cassandra table. This logger can be run either synchronously, or
  #                         asynchronously. Audit logs are stored in the dse_audit.audit_log table.
  #                         When run synchronously, a query will not execute until it has been written
  #                         to the audit log table successfully. If there is a failure between when an audit event is
  #                         written, and it's query is executed, the audit logs may contain queries that were never
  #                         executed.
  #   SLF4JAuditWriter:     logs audit info to an slf4j logger. The logger name is `SLF4JAuditWriter`, and can be configured
  #                         in the logback.xml file.
  logger: SLF4JAuditWriter

  # Comma separated list of audit event categories to be included or excluded from the audit log.
  # Categories are: QUERY, DML, DDL, DCL, AUTH, ADMIN
  # Specify either included or excluded categories. Specifying both is an error
  # included_categories:
  # excluded_categories:

  # Comma separated list of keyspaces to be included or excluded from the audit log.
  # Specify either included or excluded keyspaces. Specifying both is an error
  # included_keyspaces:
  # excluded_keyspaces:

  # The amount of time, in hours, audit events are retained by supporting loggers
  # Currently, only the CassandraAuditWriter supports retention time
  # values of 0 or less retain events forever
  retention_time: 0

  cassandra_audit_writer_options:
    # sets the mode the writer runs in.
    #
    # When run synchonously, a query is not executed until the audit event is successfully written.
    #
    # When run asynchronously, audit events are queued for writing to the audit table, but are
    # not neccesarily logged before the query executes. A pool of writer threads consumes the
    # audit events from the queue, and writes them to the audit table in batch queries. While
    # this substantially improves performance under load, if there is a failure between when
    # a query is executed, and it's audit event is written to the table, the audit table may
    # be missing entries for queries that were executed.
    # valid options are 'sync' and 'async'
    mode: sync

    # The maxium number of events the writer will dequeue before writing them out to the table. If you're seeing
    # warnings in your logs about batches being too large, decrease this value. Increasing batch_size_warn_threshold_in_kb
    # in cassandra.yaml is also an option, but make sure you understand the implications before doing so.
    #
    # Only used in async mode. Must be >0
    batch_size: 50

    # The maximum amount of time in milliseconds an event will be dequeued by a writer before being written out. This
    # prevents events from waiting too long before being written to the table when there's not a lot of queries happening.
    #
    # Only used in async mode. Must be >0
    flush_time: 500

    # The number of worker threads asynchronously logging events to the CassandraAuditWriter.
    #
    # Only used in async mode. Must be >0
    num_writers: 10

    # The size of the queue feeding the asynchronous audit log writer threads. When there are more events being
    # produced than the writers can write out, the queue will fill up, and newer queries will block until there
    # is space on the queue.
    # If a value of 0 is used, the queue size will be unbounded, which can lead to resource exhaustion under
    # heavy query load.
    queue_size: 10000

    # the consistency level used to write audit events
    write_consistency: QUORUM

# if enabled, system tables that may contain sensitive information (system.hints,
# system.batchlog, system.paxos) are encrypted with the encryption settings below.
# When enabling system table encryption on a node with existing data, run
# `nodetool upgradesstables -a` on the listed tables to encrypt existing data
#
# When tracing is enabled, sensitive info will be written into the tables in the
# system_traces keyspace. Those tables should be configured to encrypt their data
# on disk by using an encrypting compressor.
system_info_encryption:
  enabled: false
  cipher_algorithm: AES
  secret_key_strength: 128
  chunk_length_kb: 64
  # the name of the keys file that will be created to encrypt system tables.
  # This file will be created at <system_key_directory>/system/<key_name>
  key_name: system_table_keytab

  # selects an alternate key provider for local encryption. Useful for
  # using a kmip host as a key provider
  # key_provider: KmipKeyProviderFactory

  # if  KmipKeyProviderFactory is used for system_info_encryption, this specified
  # the kmip host to be used
  # kmip_host: kmip_host_namet

# Retries setting when hive inserts data to C* table. insert_max_retries is max number of retries
# insert_retry_sleep_period is the period of time in milliseconds between retries
hive_options:
  insert_max_retries: 6
  insert_retry_sleep_period: 50

# Connection settings for key servers supporting the kmip protocol
# this allows DSE's encryption features to use keys that are not stored
# on the same machine running DSE.
#
# Hosts are configured as <kmip_host_name> : <connection_settings>, which maps a user definable
# name to a set of hosts, truststores, etc used with a particular key server. This name is then
# used when referring to kmip hosts. DSE supports multiple kmip hosts.
# kmip_hosts:
  # the unique name of this kmip host/cluster which is specified in the table schema
  # kmip_host_name:
    # comma separated list of kmip hosts host[:port]
    # The current implementation of KMIP connection management only supports failover, so all requests will
    # go through a single KMIP server. There is no load balancing. This is because there aren't any KMIP servers
    # available (that we've found) that support read replication, or other strategies for availability.
    #
    # Hosts are tried in the order they appear here. So add them in the same sequence they'll fail over in
    # hosts: kmip1.yourdomain.com, kmip2.yourdomain.com

    # keystore/truststore info
    # keystore_path: /path/to/keystore.jks
    # keystore_type: jks
    # keystore_password: password
    # truststore_path: /path/to/truststore.jks
    # truststore_type: jks
    # truststore_password: password

    # Keys read from the KMIP hosts are cached locally for the period of time specified below.
    # The longer keys are cached, the fewer requests are made to the key server, but the longer
    # it takes for changes (ie: revokation) to propagate to the DSE node
    # key_cache_millis: 300000

    # socket timeout in milliseconds
    # timeout: 1000
